{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6c516b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def check_gpu():\n",
    "    import tensorflow as tf\n",
    "    print(\"tf.test.is_built_with_cuda()\")\n",
    "    print(tf.test.is_built_with_cuda())\n",
    "    \n",
    "    print()\n",
    "    print(\"tf.config.list_physical_devices('GPU')\")\n",
    "    print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "    print()\n",
    "    print(\"tf.config.experimental.list_physical_devices('GPU')\")\n",
    "    print(tf.config.experimental.list_physical_devices('GPU'))\n",
    "\n",
    "check_gpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38a7403-cdb7-4d4b-aaaf-5ddc929d1143",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def free_gpu_memory():\n",
    "    from numba import cuda\n",
    "    import gc\n",
    "    try:\n",
    "        del(model)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del(history)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        del(tf)\n",
    "    except:\n",
    "        pass\n",
    "    gc.collect()\n",
    "    cuda.select_device(0)\n",
    "    cuda.close()\n",
    "    \n",
    "free_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0535e6-e516-4ceb-9783-6e637bbed02e",
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data/train into data/all_train_images, if data/all_train_images doesnt exist\n",
    "# Load data/test into data/all_test_images, if data/all_test_images doesnt exist\n",
    "\n",
    "import os\n",
    "\n",
    "def load_into_all_images(dir_from, dir_to):\n",
    "    if os.path.exists(dir_to):\n",
    "        return \n",
    "    import shutil\n",
    "    counter = 0\n",
    "    os.makedirs(dir_to, exist_ok=True) # Create dir_to folder\n",
    "    for subdir, dirs, files in os.walk(dir_from):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(subdir, file)\n",
    "            shutil.copy(full_path, dir_to)\n",
    "            counter = counter + 1\n",
    "            if counter % 1000 == 0:\n",
    "                print(counter)\n",
    "    print(counter)\n",
    "\n",
    "load_into_all_images(\"data/train\", \"data/all_train_images\")\n",
    "load_into_all_images(\"data/test\", \"data/all_test_images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3354fdea-1e6a-4445-8eb5-1a593578b725",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load filenames_shuffled (length 14034) and y_labels_one_hot_shuffled from its files\n",
    "# If files not found, build those 2 variables now and save them to their respective file\n",
    "# Build test_filenames and test_labels\n",
    "\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "def make_filenames_and_labels(all_folder, folder):\n",
    "    subdirs, dirs, files = os.walk(all_folder).__next__()\n",
    "    m = len(files)\n",
    "    filenames = []\n",
    "    labels = np.zeros((m, 1))\n",
    "    filenames_counter = 0\n",
    "    labels_counter = -1\n",
    "    for subdir, dirs, files in os.walk(folder):\n",
    "        for file in files:\n",
    "            filenames.append(file)\n",
    "            labels[filenames_counter, 0] = labels_counter\n",
    "            filenames_counter = filenames_counter + 1\n",
    "        labels_counter = labels_counter+1\n",
    "    return filenames, labels\n",
    "        \n",
    "def load_shuffle_save():\n",
    "    from sklearn.utils import shuffle\n",
    "\n",
    "    filenames, labels = make_filenames_and_labels(\"data/all_train_images\", \"data/train\")\n",
    "    \n",
    "    # One hot vector representation of labels\n",
    "    y_labels_one_hot = to_categorical(labels)\n",
    "    \n",
    "    # Shuffle\n",
    "    filenames_shuffled, y_labels_one_hot_shuffled = shuffle(filenames, y_labels_one_hot)\n",
    "    \n",
    "    np.save('y_labels_one_hot_shuffled.npy', y_labels_one_hot_shuffled)\n",
    "    np.save('filenames_shuffled.npy', filenames_shuffled)\n",
    "    \n",
    "    return filenames_shuffled, y_labels_one_hot_shuffled\n",
    "\n",
    "if os.path.exists(\"y_labels_one_hot_shuffled.npy\") and os.path.exists(\"filenames_shuffled.npy\"):\n",
    "    print(\"Loading filenames and labels from disk\")\n",
    "    # load shuffled train data (14034)\n",
    "    y_labels_one_hot_shuffled = np.load(\"y_labels_one_hot_shuffled.npy\")\n",
    "    filenames_shuffled = np.load(\"filenames_shuffled.npy\")\n",
    "else:    \n",
    "    print(\"Making filenames and labels\")\n",
    "    filenames_shuffled, y_labels_one_hot_shuffled = load_shuffle_save()\n",
    "\n",
    "test_filenames, test_labels = make_filenames_and_labels(\"data/all_test_images\", \"data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab34299b-9ada-44cf-bf4c-21239e39814e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split train images (14034) into training set and validation set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Used this line as our filename array is not a numpy array.\n",
    "filenames_shuffled_numpy = np.array(filenames_shuffled)\n",
    "\n",
    "X_train_filenames, X_val_filenames, y_train, y_val = train_test_split(\n",
    "    filenames_shuffled_numpy, y_labels_one_hot_shuffled, test_size=0.2, random_state=1)\n",
    "\n",
    "print(\"y_train.shape\", y_train.shape) # (11227, 6)\n",
    "print(\"y_val.shape\", y_val.shape) # (2807, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2cb44-6cf0-4151-a869-799df5628869",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This image generator generates batch_size images whenever called and also their labels\n",
    "\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class My_Custom_Generator(Sequence) :\n",
    "    def __init__(self, folder, image_filenames, labels, batch_size) :\n",
    "        self.folder = folder\n",
    "        self.image_filenames = image_filenames\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self) :\n",
    "        return (np.ceil(len(self.image_filenames) / float(self.batch_size))).astype(int)\n",
    "\n",
    "    def __getitem__(self, idx) :\n",
    "        filenames_batch = self.image_filenames[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "        labels_batch = self.labels[idx * self.batch_size : (idx+1) * self.batch_size]\n",
    "\n",
    "        imgs = [resize(imread(self.folder + \"/\" + str(file_name)), (150, 150, 3)) for file_name in filenames_batch]\n",
    "        imgs = np.array(imgs)/255.0\n",
    "\n",
    "        return imgs, np.array(labels_batch)\n",
    "\n",
    "test_generator = My_Custom_Generator(\"data/all_test_images\", test_filenames, test_labels, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d0a321",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Functions for plotting learning curves and confusion matrix\n",
    "\n",
    "import matplotlib.pyplot as plt        \n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_loss_accuracy(history):\n",
    "    if not isinstance(history, dict):\n",
    "        history = history.history\n",
    "        \n",
    "    num_epochs = len(history[\"accuracy\"])\n",
    "    x_values = range(1, num_epochs + 1)  # Generate x-axis values starting from 1\n",
    "        \n",
    "    # Plot accuracy\n",
    "    plt.plot(x_values, history['accuracy'], label = \"Train acc\")\n",
    "    plt.plot(x_values, history['val_accuracy'], label = \"Validation acc\")\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.xticks(range(0, num_epochs + 1, 2), range(0, num_epochs + 1, 2))\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot loss function\n",
    "    plt.plot(x_values, history['loss'], label = \"Train loss\")\n",
    "    plt.plot(x_values, history['val_loss'], label = \"Validation loss\")\n",
    "    plt.title(\"Learning curve\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.xticks(range(0, num_epochs + 1, 2), range(0, num_epochs + 1, 2))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(model, verbose=1):\n",
    "    class_names = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\" ]\n",
    "    preds = model.predict(test_generator, workers=8, verbose=verbose)\n",
    "    preds_labels = np.argmax(preds, axis=1)\n",
    "    cm = confusion_matrix(test_labels, preds_labels)\n",
    "    ax = plt.axes()\n",
    "    sn.heatmap(cm, annot=True, fmt=\"d\",\n",
    "               annot_kws={\"size\": 10}, \n",
    "               xticklabels=class_names, \n",
    "               yticklabels=class_names, ax = ax)\n",
    "    ax.set_title('Confusion matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704f2caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Util functions for models (saving, loading, evaluating on test set)\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "def save_model_and_history(model, model_file, history, history_file):\n",
    "    print(\"Saving \" + model_file + \" and history\")\n",
    "    model.save(model_file)\n",
    "    with open(history_file, 'wb') as hist:\n",
    "        pickle.dump(history.history, hist)  \n",
    "    print(\"Saved \" + model_file + \" and history\")\n",
    "\n",
    "def load_model_and_history(model_file, model_history_file):\n",
    "    if not os.path.exists(model_file) or not os.path.exists(model_history_file):\n",
    "        return None, None\n",
    "    \n",
    "    print(\"Loading model \" + model_file + \" and history\")\n",
    "    model = load_model(model_file)\n",
    "    with open(model_history_file, \"rb\") as hist_file:\n",
    "        history = pickle.load(hist_file)\n",
    "    print(\"Loaded model \" + model_file + \" and history\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def get_test_acc(model, verbose=1):  \n",
    "    preds = model.predict(test_generator, workers=8, verbose=verbose)\n",
    "    preds_labels = np.argmax(preds, axis=1)\n",
    "    cm = confusion_matrix(test_labels, preds_labels) \n",
    "    \n",
    "    sum_diagonal = np.trace(cm)\n",
    "    acc = sum_diagonal / np.sum(cm)\n",
    "    return acc\n",
    "\n",
    "def get_test_accs(model, verbose=1):\n",
    "    preds = model.predict(test_generator, workers=8, verbose=verbose)\n",
    "    preds_labels = np.argmax(preds, axis=1)\n",
    "    cm = confusion_matrix(test_labels, preds_labels) \n",
    "    \n",
    "    accuracies = []\n",
    "    for i in range(cm.shape[0]):\n",
    "        tp = cm[i,i]\n",
    "        fp = np.sum(cm[:, i]) - tp\n",
    "        tn = np.sum(cm) - np.sum(cm[:, i]) - np.sum(cm[i, :]) + tp\n",
    "        fn = np.sum(cm[i, :]) - tp\n",
    "        acc_class = (tp + tn) / (tp+fp+tn+fn)\n",
    "        accuracies.append(acc_class)\n",
    "        \n",
    "    return accuracies\n",
    "\n",
    "def evaluate_model(model):\n",
    "    print(\"Evaluating on test set...\")\n",
    "    acc = get_test_acc(model, verbose=0)\n",
    "    print(\"Test set accuracy: {:.2f}%\".format(acc*100))\n",
    "    accs = get_test_accs(model, verbose=0)\n",
    "    classes = [\"buildings\", \"forest\", \"glacier\", \"mountain\", \"sea\", \"street\"]\n",
    "    for i in range(len(classes)):\n",
    "        print(\"Test set accuracy for {}: {:.2f}%\".format(classes[i], accs[i]*100))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c754442-6c1c-4ae5-a481-86ec629e8691",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# CNNs\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.nn import *\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50, DenseNet121,  VGG16, InceptionV3\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def train_model(model_name, batch_size, learning_rate=0.001, show_summary=True):\n",
    "    if model_name == \"cnn1\":\n",
    "        model = Sequential([\n",
    "            Conv2D(32, (3, 3), activation = 'relu', input_shape = (150, 150, 3)), \n",
    "            MaxPooling2D(2,2),\n",
    "            Conv2D(32, (3, 3), activation = 'relu'),\n",
    "            MaxPooling2D(2,2),\n",
    "            Flatten(),\n",
    "            Dense(64, activation=relu),\n",
    "            Dense(6, activation=softmax)\n",
    "        ])\n",
    "    elif model_name == \"cnn2\":\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(32,(3,3), input_shape=(150, 150,3), activation='relu'))\n",
    "        model.add(MaxPool2D(pool_size=(2,2),strides=None))\n",
    "        model.add(Conv2D(64,(3,3), activation='relu' , padding= \"same\"))\n",
    "        model.add(MaxPool2D(pool_size=(2,2),strides=None))\n",
    "        model.add(Conv2D(64,(3,3), activation='relu' , padding= \"same\"))\n",
    "        model.add(MaxPool2D(pool_size=(2,2),strides=None))\n",
    "        model.add(Conv2D(128,(3,3), activation='relu' , padding= \"same\"))\n",
    "        model.add(MaxPool2D(pool_size=(2,2),strides=None))\n",
    "        model.add(Conv2D(128,(3,3), activation='relu' , padding= \"same\"))\n",
    "        model.add(MaxPool2D(pool_size=(2,2),strides=None))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.50))\n",
    "        model.add(Dense(6,activation ='softmax'))\n",
    "    elif model_name == \"resnet\":\n",
    "        # Load the pre-trained ResNet-50 model\n",
    "        base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "        # Freeze the base model's layers\n",
    "        base_model.trainable = False\n",
    "        # Create a new model on top of the base model\n",
    "        model = Sequential([\n",
    "            base_model,\n",
    "            GlobalAveragePooling2D(),\n",
    "            Dropout(0.25),\n",
    "            Dense(6, activation='softmax')\n",
    "        ])\n",
    "    elif model_name == \"densenet\":\n",
    "        base_model = DenseNet121(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "        # Freeze the base model's layers\n",
    "        base_model.trainable = False\n",
    "        # Create a new model on top of the base model\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(6, activation='softmax'))  # Assuming 6 scene classes\n",
    "    elif model_name == \"vgg16\":\n",
    "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "        # Freeze the base model's layers\n",
    "        base_model.trainable = False\n",
    "        # Create a new model on top of the base model\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(6, activation='softmax'))  # Assuming 6 scene classes\n",
    "    elif model_name == \"inception\":\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n",
    "        # Freeze the base model's layers\n",
    "        base_model.trainable = False\n",
    "        # Create a new model on top of the base model\n",
    "        model = Sequential()\n",
    "        model.add(base_model)\n",
    "        model.add(GlobalAveragePooling2D())\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dense(6, activation='softmax'))  # Assuming 6 scene classes\n",
    "    else:\n",
    "        return None, None # model, history\n",
    "    \n",
    "    if show_summary:\n",
    "        model.summary()\n",
    "                                 \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate), \n",
    "        loss='categorical_crossentropy', \n",
    "        metrics=['accuracy'])\n",
    "    \n",
    "    my_training_batch_generator = My_Custom_Generator(\"data/all_train_images\", X_train_filenames, y_train, batch_size)\n",
    "    my_validation_batch_generator = My_Custom_Generator(\"data/all_train_images\", X_val_filenames, y_val, batch_size)\n",
    "    \n",
    "    # Stop training if val_accuracy < 0.2 after more than 3 epochs\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, mode='max', min_delta=0.0, baseline=0.2)\n",
    "    \n",
    "    history = model.fit(\n",
    "        my_training_batch_generator,\n",
    "        steps_per_epoch=int(11227 // batch_size),\n",
    "        epochs=20,\n",
    "        verbose=1,\n",
    "        validation_data=my_validation_batch_generator,\n",
    "        validation_steps=int(2807 // batch_size),\n",
    "        callbacks=[early_stopping],\n",
    "        workers=64\n",
    "    )\n",
    "        \n",
    "    return model, history\n",
    "\n",
    "def train_model_load_save_and_analysis(model_name, batch_size, learning_rate, model_file, show_summary=True):\n",
    "    print(\"model_name={0}, batch_size={1}, learning_rate={2}, model_file={3}\"\n",
    "          .format(model_name, batch_size, learning_rate, model_file))\n",
    "          \n",
    "    history_file = model_file[0:-3] + \"_history\"\n",
    "    model, history = load_model_and_history(model_file, history_file)\n",
    "    was_model_loaded = model != None and history != None\n",
    "\n",
    "    if was_model_loaded:\n",
    "        if show_summary:\n",
    "            model.summary()\n",
    "        acc = evaluate_model(model)\n",
    "    else:\n",
    "        # Couldn't load model, so train it now and save it\n",
    "        model, history = train_model(model_name=model_name,\n",
    "                                     batch_size=batch_size,\n",
    "                                     learning_rate=learning_rate,\n",
    "                                     show_summary=show_summary)\n",
    "        save_model_and_history(model, model_file, history, history_file)\n",
    "        acc = evaluate_model(model)\n",
    "        \n",
    "    if acc < 0.5:\n",
    "        print(\"Accuracy < 0.5, skipping confusion matrix and learning curves\")\n",
    "        return model, history\n",
    "        \n",
    "    plot_confusion_matrix(model, verbose=0)\n",
    "    plot_loss_accuracy(history)  \n",
    "    \n",
    "    return model, history\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217e819b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the models folder if doesnt exist\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6025f9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Grid search, for each CNN\n",
    "\n",
    "models_names = [\"cnn1\", \"cnn2\", \"resnet\", \"densenet\", \"vgg16\", \"inception\"]\n",
    "batch_sizes = [32, 128]\n",
    "learning_rates = [0.01, 0.001, 0.0001]\n",
    "\n",
    "for model_name in models_names:\n",
    "    for batch_size in batch_sizes:\n",
    "        for learning_rate in learning_rates:\n",
    "            model_file = \"models/{0}_{1}_{2}.h5\".format(model_name, batch_size, learning_rate)\n",
    "            show_summary = batch_size == batch_sizes[0] and learning_rate == learning_rates[0]\n",
    "            train_model_load_save_and_analysis(model_name, \n",
    "                                               batch_size, \n",
    "                                               learning_rate, \n",
    "                                               model_file, \n",
    "                                               show_summary=show_summary)  \n",
    "            printSeparator = model_name != models_names[-1] or batch_size != batch_sizes[-1] or learning_rate != learning_rates[-1]\n",
    "            if printSeparator:\n",
    "                print(\"--------//--------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881e81fd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "free_gpu_memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83782532",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "**CNN 1 - Test set accuracies**\n",
    "\n",
    "18.43% model_name=cnn1, batch_size=32, learning_rate=0.01, model_file=models/cnn1_32_0.01.h5\n",
    "\n",
    "17.50% model_name=cnn1, batch_size=32, learning_rate=0.001, model_file=models/cnn1_32_0.001.h5\n",
    "\n",
    "62.33% model_name=cnn1, batch_size=32, learning_rate=0.0001, model_file=models/cnn1_32_0.0001.h5\n",
    "\n",
    "17.50% model_name=cnn1, batch_size=128, learning_rate=0.01, model_file=models/cnn1_128_0.01.h5\n",
    "\n",
    "65.8% model_name=cnn1, batch_size=128, learning_rate=0.001, model_file=models/cnn1_128_0.001.h5\n",
    "\n",
    "59.27% model_name=cnn1, batch_size=128, learning_rate=0.0001, model_file=models/cnn1_128_0.0001.h5\n",
    "\n",
    "**CNN 2 - Test set accuracies**\n",
    "\n",
    "17.50% model_name=cnn2, batch_size=32, learning_rate=0.01, model_file=models/cnn2_32_0.01.h5\n",
    "\n",
    "17.50% model_name=cnn2, batch_size=32, learning_rate=0.001, model_file=models/cnn2_32_0.001.h5\n",
    "\n",
    "70.8% model_name=cnn2, batch_size=32, learning_rate=0.0001, model_file=models/cnn2_32_0.0001.h5\n",
    "\n",
    "18.43% model_name=cnn2, batch_size=128, learning_rate=0.01, model_file=models/cnn2_128_0.01.h5\n",
    "\n",
    "72.97% model_name=cnn2, batch_size=128, learning_rate=0.001, model_file=models/cnn2_128_0.001.h5\n",
    "\n",
    "65.50% model_name=cnn2, batch_size=128, learning_rate=0.0001, model_file=models/cnn2_128_0.0001.h5\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
